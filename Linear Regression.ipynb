{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-A: Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of features in train and test data are\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.DataFrame(pd.read_csv(\"train_wbcd.csv\"))\n",
    "testData  = pd.DataFrame(pd.read_csv(\"test_wbcd.csv\"))\n",
    "trainClass = trainData['Diagnosis']\n",
    "testClass = testData['Diagnosis']\n",
    "print('No of features in train and test data are')\n",
    "print(len(list(trainData))-2)\n",
    "print(len(list(testData))-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LengthOfClassB 58\n",
      "LengthOfClassM 42\n"
     ]
    }
   ],
   "source": [
    "DataClassB = trainData[trainData['Diagnosis']=='B']\n",
    "DataClassM = trainData[trainData['Diagnosis']=='M']\n",
    "\n",
    "print(\"LengthOfClassB\",len(DataClassB))\n",
    "print(\"LengthOfClassM\",len(DataClassM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainData.drop(['Diagnosis','Patient_ID'],axis=1).copy()\n",
    "testData = testData.drop(['Diagnosis','Patient_ID'],axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the ratio of both classes of data is 42/58 is 0.7 so data is fairly balanced. If this ratio would have been below 0.5 then we would have classified this data as unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features with missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_data = trainData[trainData.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>13.62</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.02974</td>\n",
       "      <td>0.02443</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       f1     f2     f3     f4       f5       f6       f7       f8      f9  \\\n",
       "70  13.62  23.23  87.19  573.2  0.09246  0.06747  0.02974  0.02443  0.1664   \n",
       "74  12.45  15.70  82.57  477.1  0.12780  0.17000  0.15780  0.08089  0.2087   \n",
       "\n",
       "        f10   ...     f21    f22     f23    f24     f25     f26     f27  \\\n",
       "70  0.05801   ...     NaN  29.09   97.58  729.8  0.1216  0.1517  0.1049   \n",
       "74  0.07613   ...     NaN  23.75  103.40  741.6  0.1791  0.5249  0.5355   \n",
       "\n",
       "        f28     f29      f30  \n",
       "70  0.07174  0.2642  0.06953  \n",
       "74  0.17410  0.3985  0.12440  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(null_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see f21 has got NaN values so it is a feature with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the missing feature values with mean of the respective feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = trainData.fillna(trainData.mean())\n",
    "testData = testData.fillna(testData.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Training and Testing Data(Z-Score Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test Data Normalized!!\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(trainData.T)):\n",
    "    trainData.T.iloc[i] = ((trainData.T.iloc[i] - np.mean(trainData.T.iloc[i]))/np.std(trainData.T.iloc[i]))\n",
    "    \n",
    "for i in range(0,len(testData.T)):\n",
    "    testData.T.iloc[i] = ((testData.T.iloc[i] - np.mean(testData.T.iloc[i]))/np.std(testData.T.iloc[i]))\n",
    "\n",
    "print('Train and Test Data Normalized!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.087488</td>\n",
       "      <td>-0.587788</td>\n",
       "      <td>-1.079973</td>\n",
       "      <td>-0.785110</td>\n",
       "      <td>0.724397</td>\n",
       "      <td>-0.923772</td>\n",
       "      <td>-0.936765</td>\n",
       "      <td>-0.956143</td>\n",
       "      <td>0.915178</td>\n",
       "      <td>0.833759</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.102548</td>\n",
       "      <td>-0.900914</td>\n",
       "      <td>-1.065709</td>\n",
       "      <td>-0.726083</td>\n",
       "      <td>-0.065492</td>\n",
       "      <td>-1.212458</td>\n",
       "      <td>-1.230840</td>\n",
       "      <td>-1.426554</td>\n",
       "      <td>-0.010344</td>\n",
       "      <td>-0.364117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.350978</td>\n",
       "      <td>-0.709063</td>\n",
       "      <td>-0.381589</td>\n",
       "      <td>-0.359932</td>\n",
       "      <td>0.190700</td>\n",
       "      <td>-0.909882</td>\n",
       "      <td>-0.633032</td>\n",
       "      <td>-0.153026</td>\n",
       "      <td>-0.885389</td>\n",
       "      <td>-0.341121</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.368690</td>\n",
       "      <td>-0.745160</td>\n",
       "      <td>-0.406849</td>\n",
       "      <td>-0.362366</td>\n",
       "      <td>-0.810933</td>\n",
       "      <td>-1.198650</td>\n",
       "      <td>-0.942635</td>\n",
       "      <td>-0.407719</td>\n",
       "      <td>-1.333492</td>\n",
       "      <td>-0.906747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.332350</td>\n",
       "      <td>-0.498150</td>\n",
       "      <td>-0.364556</td>\n",
       "      <td>-0.366212</td>\n",
       "      <td>-0.567675</td>\n",
       "      <td>-0.705465</td>\n",
       "      <td>-0.634842</td>\n",
       "      <td>-0.637866</td>\n",
       "      <td>-1.170038</td>\n",
       "      <td>-0.593194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.378679</td>\n",
       "      <td>-0.482838</td>\n",
       "      <td>-0.404070</td>\n",
       "      <td>-0.370835</td>\n",
       "      <td>-0.303323</td>\n",
       "      <td>-0.476537</td>\n",
       "      <td>-0.538112</td>\n",
       "      <td>-0.442866</td>\n",
       "      <td>-0.700336</td>\n",
       "      <td>-0.419691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.899344</td>\n",
       "      <td>0.229500</td>\n",
       "      <td>-0.895942</td>\n",
       "      <td>-0.683829</td>\n",
       "      <td>0.517002</td>\n",
       "      <td>-0.859088</td>\n",
       "      <td>-0.705204</td>\n",
       "      <td>-0.668940</td>\n",
       "      <td>0.458416</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.786563</td>\n",
       "      <td>0.541860</td>\n",
       "      <td>-0.785954</td>\n",
       "      <td>-0.587673</td>\n",
       "      <td>0.569907</td>\n",
       "      <td>-0.934778</td>\n",
       "      <td>-0.941646</td>\n",
       "      <td>-0.890100</td>\n",
       "      <td>-0.317260</td>\n",
       "      <td>-0.279176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.175506</td>\n",
       "      <td>-0.034142</td>\n",
       "      <td>-1.107360</td>\n",
       "      <td>-0.820153</td>\n",
       "      <td>-0.198512</td>\n",
       "      <td>0.569547</td>\n",
       "      <td>0.557579</td>\n",
       "      <td>-0.460792</td>\n",
       "      <td>1.110460</td>\n",
       "      <td>1.822324</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.079240</td>\n",
       "      <td>0.292654</td>\n",
       "      <td>-1.032592</td>\n",
       "      <td>-0.714829</td>\n",
       "      <td>0.939077</td>\n",
       "      <td>1.144315</td>\n",
       "      <td>1.303197</td>\n",
       "      <td>-0.305409</td>\n",
       "      <td>0.194266</td>\n",
       "      <td>2.682921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.126369</td>\n",
       "      <td>0.769964</td>\n",
       "      <td>0.171506</td>\n",
       "      <td>-0.022262</td>\n",
       "      <td>2.445778</td>\n",
       "      <td>1.529434</td>\n",
       "      <td>0.825679</td>\n",
       "      <td>1.140414</td>\n",
       "      <td>1.206446</td>\n",
       "      <td>0.901709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.603571</td>\n",
       "      <td>2.281387</td>\n",
       "      <td>0.598231</td>\n",
       "      <td>0.333048</td>\n",
       "      <td>3.054708</td>\n",
       "      <td>1.743964</td>\n",
       "      <td>0.957069</td>\n",
       "      <td>1.437852</td>\n",
       "      <td>1.456031</td>\n",
       "      <td>0.875662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.861622</td>\n",
       "      <td>-1.296982</td>\n",
       "      <td>-0.875568</td>\n",
       "      <td>-0.666813</td>\n",
       "      <td>-0.849042</td>\n",
       "      <td>-1.288653</td>\n",
       "      <td>-0.749887</td>\n",
       "      <td>-0.575262</td>\n",
       "      <td>-1.110460</td>\n",
       "      <td>-0.213988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.843167</td>\n",
       "      <td>-1.438676</td>\n",
       "      <td>-0.855198</td>\n",
       "      <td>-0.614938</td>\n",
       "      <td>0.491813</td>\n",
       "      <td>-1.095863</td>\n",
       "      <td>-0.893612</td>\n",
       "      <td>-0.398897</td>\n",
       "      <td>-0.447983</td>\n",
       "      <td>-0.336105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.295483</td>\n",
       "      <td>1.434340</td>\n",
       "      <td>3.355815</td>\n",
       "      <td>3.832896</td>\n",
       "      <td>0.793529</td>\n",
       "      <td>1.985535</td>\n",
       "      <td>3.175228</td>\n",
       "      <td>2.902929</td>\n",
       "      <td>0.577572</td>\n",
       "      <td>-1.068845</td>\n",
       "      <td>...</td>\n",
       "      <td>3.407147</td>\n",
       "      <td>0.563174</td>\n",
       "      <td>3.439783</td>\n",
       "      <td>3.930776</td>\n",
       "      <td>-0.029995</td>\n",
       "      <td>1.108296</td>\n",
       "      <td>1.986977</td>\n",
       "      <td>2.308699</td>\n",
       "      <td>-0.579844</td>\n",
       "      <td>-0.675418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.314979</td>\n",
       "      <td>0.841147</td>\n",
       "      <td>0.272039</td>\n",
       "      <td>0.109201</td>\n",
       "      <td>-0.496470</td>\n",
       "      <td>-0.351779</td>\n",
       "      <td>-0.585860</td>\n",
       "      <td>-0.280520</td>\n",
       "      <td>-0.666938</td>\n",
       "      <td>-0.802524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089139</td>\n",
       "      <td>0.192643</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>-0.072204</td>\n",
       "      <td>-0.889026</td>\n",
       "      <td>-0.552577</td>\n",
       "      <td>-0.798108</td>\n",
       "      <td>-0.438170</td>\n",
       "      <td>-0.715114</td>\n",
       "      <td>-0.757196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.646892</td>\n",
       "      <td>0.179408</td>\n",
       "      <td>1.652440</td>\n",
       "      <td>1.327206</td>\n",
       "      <td>1.388062</td>\n",
       "      <td>1.751265</td>\n",
       "      <td>1.964820</td>\n",
       "      <td>2.480237</td>\n",
       "      <td>2.258983</td>\n",
       "      <td>0.078636</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619118</td>\n",
       "      <td>0.643510</td>\n",
       "      <td>1.584784</td>\n",
       "      <td>1.243792</td>\n",
       "      <td>0.804188</td>\n",
       "      <td>1.265712</td>\n",
       "      <td>1.285773</td>\n",
       "      <td>2.394077</td>\n",
       "      <td>2.735984</td>\n",
       "      <td>0.595537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.332350</td>\n",
       "      <td>-0.561424</td>\n",
       "      <td>-0.283729</td>\n",
       "      <td>-0.340689</td>\n",
       "      <td>0.966358</td>\n",
       "      <td>1.006990</td>\n",
       "      <td>0.441064</td>\n",
       "      <td>0.227854</td>\n",
       "      <td>1.371939</td>\n",
       "      <td>1.315986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207342</td>\n",
       "      <td>0.030331</td>\n",
       "      <td>0.387488</td>\n",
       "      <td>0.026412</td>\n",
       "      <td>0.963925</td>\n",
       "      <td>2.121496</td>\n",
       "      <td>2.044901</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>1.830014</td>\n",
       "      <td>2.303397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.043613</td>\n",
       "      <td>-1.349711</td>\n",
       "      <td>-0.026553</td>\n",
       "      <td>-0.179855</td>\n",
       "      <td>0.731310</td>\n",
       "      <td>0.496985</td>\n",
       "      <td>-0.420023</td>\n",
       "      <td>-0.245562</td>\n",
       "      <td>0.266444</td>\n",
       "      <td>0.233168</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178899</td>\n",
       "      <td>-1.220620</td>\n",
       "      <td>-0.152337</td>\n",
       "      <td>-0.273148</td>\n",
       "      <td>-0.189732</td>\n",
       "      <td>0.121110</td>\n",
       "      <td>-0.340795</td>\n",
       "      <td>-0.390217</td>\n",
       "      <td>0.037398</td>\n",
       "      <td>-0.333846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.609443</td>\n",
       "      <td>1.157516</td>\n",
       "      <td>-0.615052</td>\n",
       "      <td>-0.530287</td>\n",
       "      <td>-0.024300</td>\n",
       "      <td>-0.636221</td>\n",
       "      <td>-0.667647</td>\n",
       "      <td>-0.587143</td>\n",
       "      <td>0.034754</td>\n",
       "      <td>-0.295090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.553486</td>\n",
       "      <td>1.189469</td>\n",
       "      <td>-0.571738</td>\n",
       "      <td>-0.475948</td>\n",
       "      <td>0.474065</td>\n",
       "      <td>-0.798040</td>\n",
       "      <td>-0.701051</td>\n",
       "      <td>-0.553429</td>\n",
       "      <td>0.293161</td>\n",
       "      <td>-0.350111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.294022</td>\n",
       "      <td>-0.392693</td>\n",
       "      <td>0.261351</td>\n",
       "      <td>0.103124</td>\n",
       "      <td>-0.901582</td>\n",
       "      <td>-0.518878</td>\n",
       "      <td>0.060634</td>\n",
       "      <td>-0.287831</td>\n",
       "      <td>-1.368630</td>\n",
       "      <td>-0.934040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120770</td>\n",
       "      <td>0.074598</td>\n",
       "      <td>0.125797</td>\n",
       "      <td>-0.041111</td>\n",
       "      <td>-0.981319</td>\n",
       "      <td>0.036399</td>\n",
       "      <td>0.548778</td>\n",
       "      <td>-0.061372</td>\n",
       "      <td>-0.773087</td>\n",
       "      <td>-0.659152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.356892</td>\n",
       "      <td>-0.835611</td>\n",
       "      <td>0.315792</td>\n",
       "      <td>0.133711</td>\n",
       "      <td>-0.354750</td>\n",
       "      <td>-0.292900</td>\n",
       "      <td>-0.476471</td>\n",
       "      <td>-0.439772</td>\n",
       "      <td>0.021514</td>\n",
       "      <td>-0.780605</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142413</td>\n",
       "      <td>-0.851729</td>\n",
       "      <td>0.074848</td>\n",
       "      <td>-0.042271</td>\n",
       "      <td>-0.495008</td>\n",
       "      <td>-0.475870</td>\n",
       "      <td>-0.260738</td>\n",
       "      <td>-0.244364</td>\n",
       "      <td>0.510276</td>\n",
       "      <td>-0.739576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.263751</td>\n",
       "      <td>1.624161</td>\n",
       "      <td>0.194552</td>\n",
       "      <td>0.075576</td>\n",
       "      <td>-1.864587</td>\n",
       "      <td>-1.053139</td>\n",
       "      <td>-0.543552</td>\n",
       "      <td>-0.559268</td>\n",
       "      <td>-0.594121</td>\n",
       "      <td>-1.277079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663184</td>\n",
       "      <td>-0.052524</td>\n",
       "      <td>-0.152373</td>\n",
       "      <td>-1.236898</td>\n",
       "      <td>-0.756685</td>\n",
       "      <td>-0.537641</td>\n",
       "      <td>-0.634965</td>\n",
       "      <td>-0.916314</td>\n",
       "      <td>-1.110515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.019728</td>\n",
       "      <td>-1.631807</td>\n",
       "      <td>-0.922995</td>\n",
       "      <td>-0.738925</td>\n",
       "      <td>0.062806</td>\n",
       "      <td>1.038088</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>-0.299941</td>\n",
       "      <td>0.051303</td>\n",
       "      <td>2.610327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.972524</td>\n",
       "      <td>-2.010867</td>\n",
       "      <td>-0.928842</td>\n",
       "      <td>-0.674687</td>\n",
       "      <td>-0.562452</td>\n",
       "      <td>-0.105676</td>\n",
       "      <td>-0.555536</td>\n",
       "      <td>-0.745528</td>\n",
       "      <td>-1.019756</td>\n",
       "      <td>0.866626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.044871</td>\n",
       "      <td>1.945803</td>\n",
       "      <td>-0.017869</td>\n",
       "      <td>-0.094576</td>\n",
       "      <td>-1.503719</td>\n",
       "      <td>-1.156177</td>\n",
       "      <td>-0.792987</td>\n",
       "      <td>-0.700928</td>\n",
       "      <td>-1.540743</td>\n",
       "      <td>-0.915409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147267</td>\n",
       "      <td>1.261608</td>\n",
       "      <td>-0.128715</td>\n",
       "      <td>-0.215023</td>\n",
       "      <td>-0.913874</td>\n",
       "      <td>-0.758686</td>\n",
       "      <td>-0.857209</td>\n",
       "      <td>-0.603944</td>\n",
       "      <td>-0.519597</td>\n",
       "      <td>-0.840782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.391820</td>\n",
       "      <td>-0.542969</td>\n",
       "      <td>0.381255</td>\n",
       "      <td>0.163488</td>\n",
       "      <td>-1.074411</td>\n",
       "      <td>0.283447</td>\n",
       "      <td>0.086991</td>\n",
       "      <td>-0.142288</td>\n",
       "      <td>-0.471656</td>\n",
       "      <td>-0.115351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112446</td>\n",
       "      <td>-0.404141</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>-0.065243</td>\n",
       "      <td>-1.307893</td>\n",
       "      <td>-0.048979</td>\n",
       "      <td>-0.053532</td>\n",
       "      <td>-0.230561</td>\n",
       "      <td>-0.339995</td>\n",
       "      <td>0.133782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.022656</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>-0.033566</td>\n",
       "      <td>-0.156560</td>\n",
       "      <td>0.015105</td>\n",
       "      <td>0.034664</td>\n",
       "      <td>-0.002489</td>\n",
       "      <td>0.243848</td>\n",
       "      <td>-0.465036</td>\n",
       "      <td>-0.461677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109117</td>\n",
       "      <td>0.320526</td>\n",
       "      <td>0.065585</td>\n",
       "      <td>-0.145296</td>\n",
       "      <td>0.488263</td>\n",
       "      <td>0.873506</td>\n",
       "      <td>0.524761</td>\n",
       "      <td>0.860133</td>\n",
       "      <td>0.615991</td>\n",
       "      <td>0.314508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1        f2        f3        f4        f5        f6        f7  \\\n",
       "0  -1.087488 -0.587788 -1.079973 -0.785110  0.724397 -0.923772 -0.936765   \n",
       "1  -0.350978 -0.709063 -0.381589 -0.359932  0.190700 -0.909882 -0.633032   \n",
       "2  -0.332350 -0.498150 -0.364556 -0.366212 -0.567675 -0.705465 -0.634842   \n",
       "3  -0.899344  0.229500 -0.895942 -0.683829  0.517002 -0.859088 -0.705204   \n",
       "4  -1.175506 -0.034142 -1.107360 -0.820153 -0.198512  0.569547  0.557579   \n",
       "5   0.126369  0.769964  0.171506 -0.022262  2.445778  1.529434  0.825679   \n",
       "6  -0.861622 -1.296982 -0.875568 -0.666813 -0.849042 -1.288653 -0.749887   \n",
       "7   3.295483  1.434340  3.355815  3.832896  0.793529  1.985535  3.175228   \n",
       "8   0.314979  0.841147  0.272039  0.109201 -0.496470 -0.351779 -0.585860   \n",
       "9   1.646892  0.179408  1.652440  1.327206  1.388062  1.751265  1.964820   \n",
       "10 -0.332350 -0.561424 -0.283729 -0.340689  0.966358  1.006990  0.441064   \n",
       "11 -0.043613 -1.349711 -0.026553 -0.179855  0.731310  0.496985 -0.420023   \n",
       "12 -0.609443  1.157516 -0.615052 -0.530287 -0.024300 -0.636221 -0.667647   \n",
       "13  0.294022 -0.392693  0.261351  0.103124 -0.901582 -0.518878  0.060634   \n",
       "14  0.356892 -0.835611  0.315792  0.133711 -0.354750 -0.292900 -0.476471   \n",
       "15  0.263751  1.624161  0.194552  0.075576 -1.864587 -1.053139 -0.543552   \n",
       "16 -1.019728 -1.631807 -0.922995 -0.738925  0.062806  1.038088  0.036765   \n",
       "17  0.044871  1.945803 -0.017869 -0.094576 -1.503719 -1.156177 -0.792987   \n",
       "18  0.391820 -0.542969  0.381255  0.163488 -1.074411  0.283447  0.086991   \n",
       "19 -0.022656  0.258500 -0.033566 -0.156560  0.015105  0.034664 -0.002489   \n",
       "\n",
       "          f8        f9       f10    ...          f21       f22       f23  \\\n",
       "0  -0.956143  0.915178  0.833759    ...    -1.102548 -0.900914 -1.065709   \n",
       "1  -0.153026 -0.885389 -0.341121    ...    -0.368690 -0.745160 -0.406849   \n",
       "2  -0.637866 -1.170038 -0.593194    ...    -0.378679 -0.482838 -0.404070   \n",
       "3  -0.668940  0.458416  0.003014    ...    -0.786563  0.541860 -0.785954   \n",
       "4  -0.460792  1.110460  1.822324    ...    -1.079240  0.292654 -1.032592   \n",
       "5   1.140414  1.206446  0.901709    ...     0.603571  2.281387  0.598231   \n",
       "6  -0.575262 -1.110460 -0.213988    ...    -0.843167 -1.438676 -0.855198   \n",
       "7   2.902929  0.577572 -1.068845    ...     3.407147  0.563174  3.439783   \n",
       "8  -0.280520 -0.666938 -0.802524    ...     0.089139  0.192643  0.005373   \n",
       "9   2.480237  2.258983  0.078636    ...     1.619118  0.643510  1.584784   \n",
       "10  0.227854  1.371939  1.315986    ...     0.207342  0.030331  0.387488   \n",
       "11 -0.245562  0.266444  0.233168    ...    -0.178899 -1.220620 -0.152337   \n",
       "12 -0.587143  0.034754 -0.295090    ...    -0.553486  1.189469 -0.571738   \n",
       "13 -0.287831 -1.368630 -0.934040    ...     0.120770  0.074598  0.125797   \n",
       "14 -0.439772  0.021514 -0.780605    ...     0.142413 -0.851729  0.074848   \n",
       "15 -0.559268 -0.594121 -1.277079    ...     0.000000  0.663184 -0.052524   \n",
       "16 -0.299941  0.051303  2.610327    ...    -0.972524 -2.010867 -0.928842   \n",
       "17 -0.700928 -1.540743 -0.915409    ...    -0.147267  1.261608 -0.128715   \n",
       "18 -0.142288 -0.471656 -0.115351    ...     0.112446 -0.404141  0.102639   \n",
       "19  0.243848 -0.465036 -0.461677    ...     0.109117  0.320526  0.065585   \n",
       "\n",
       "         f24       f25       f26       f27       f28       f29       f30  \n",
       "0  -0.726083 -0.065492 -1.212458 -1.230840 -1.426554 -0.010344 -0.364117  \n",
       "1  -0.362366 -0.810933 -1.198650 -0.942635 -0.407719 -1.333492 -0.906747  \n",
       "2  -0.370835 -0.303323 -0.476537 -0.538112 -0.442866 -0.700336 -0.419691  \n",
       "3  -0.587673  0.569907 -0.934778 -0.941646 -0.890100 -0.317260 -0.279176  \n",
       "4  -0.714829  0.939077  1.144315  1.303197 -0.305409  0.194266  2.682921  \n",
       "5   0.333048  3.054708  1.743964  0.957069  1.437852  1.456031  0.875662  \n",
       "6  -0.614938  0.491813 -1.095863 -0.893612 -0.398897 -0.447983 -0.336105  \n",
       "7   3.930776 -0.029995  1.108296  1.986977  2.308699 -0.579844 -0.675418  \n",
       "8  -0.072204 -0.889026 -0.552577 -0.798108 -0.438170 -0.715114 -0.757196  \n",
       "9   1.243792  0.804188  1.265712  1.285773  2.394077  2.735984  0.595537  \n",
       "10  0.026412  0.963925  2.121496  2.044901  0.773333  1.830014  2.303397  \n",
       "11 -0.273148 -0.189732  0.121110 -0.340795 -0.390217  0.037398 -0.333846  \n",
       "12 -0.475948  0.474065 -0.798040 -0.701051 -0.553429  0.293161 -0.350111  \n",
       "13 -0.041111 -0.981319  0.036399  0.548778 -0.061372 -0.773087 -0.659152  \n",
       "14 -0.042271 -0.495008 -0.475870 -0.260738 -0.244364  0.510276 -0.739576  \n",
       "15 -0.152373 -1.236898 -0.756685 -0.537641 -0.634965 -0.916314 -1.110515  \n",
       "16 -0.674687 -0.562452 -0.105676 -0.555536 -0.745528 -1.019756  0.866626  \n",
       "17 -0.215023 -0.913874 -0.758686 -0.857209 -0.603944 -0.519597 -0.840782  \n",
       "18 -0.065243 -1.307893 -0.048979 -0.053532 -0.230561 -0.339995  0.133782  \n",
       "19 -0.145296  0.488263  0.873506  0.524761  0.860133  0.615991  0.314508  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 30)\n"
     ]
    }
   ],
   "source": [
    "trainClass = np.array(trainClass)\n",
    "trainClass = np.reshape(trainClass,(1,100))\n",
    "trainData = np.array(trainData)\n",
    "trainClassConverted = []\n",
    "print(np.shape(trainData))\n",
    "for i in range(0,len(trainClass[0])):\n",
    "    if(trainClass[0][i]=='B'):\n",
    "        trainClassConverted.append(0)\n",
    "    else:\n",
    "        trainClassConverted.append(1)\n",
    "        \n",
    "testClass = np.array(testClass)\n",
    "testClass = np.reshape(testClass,(1,20))\n",
    "testData = np.array(testData)\n",
    "testClassConverted = []\n",
    "for i in range(0,len(testClass[0])):\n",
    "    if(testClass[0][i]=='B'):\n",
    "        testClassConverted.append(0)\n",
    "    else:\n",
    "        testClassConverted.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression with L1(Lasso) RegularizationÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Root Mean Square Error:  0.3236197989057903\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "\n",
    "regressor = Lasso(random_state=0,alpha=0.1)\n",
    "regressor.fit(trainData,np.array(trainClassConverted))\n",
    "predLasso = regressor.predict(testData)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(predLasso,testClassConverted))\n",
    "\n",
    "print(\"Final Root Mean Square Error: \",(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  95.0  %\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(predLasso)):\n",
    "    if(predLasso[i]<=0.5):\n",
    "        predLasso[i] = 0\n",
    "    else:\n",
    "        predLasso[i] = 1\n",
    "print(\"Accuracy: \",(sklearn.metrics.accuracy_score(testClassConverted,predLasso))*100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score | F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predLasso1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-0a878c892204>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestClassConverted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredLasso1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"===================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precision | F1 Scores: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"===================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predLasso1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "precision = (sklearn.metrics.precision_recall_fscore_support(testClassConverted,predLasso1))\n",
    "print(\"===================\")\n",
    "print(\"Precision | F1 Scores: \")\n",
    "print(\"===================\")\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = sklearn.metrics.confusion_matrix(testClassConverted,predLasso1)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"================\")\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression with L2(Ridge) Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regressor = Ridge(alpha=0.1)\n",
    "\n",
    "regressor.fit(trainData,trainClassConverted)\n",
    "predRidge = regressor.predict(testData)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(predRidge,testClassConverted))\n",
    "\n",
    "print(\"Final Root Mean Square Error: \",(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRidge = np.around(predRidge)\n",
    "print(\"Accuracy: \",(sklearn.metrics.accuracy_score(testClassConverted,predRidge))*100,\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (sklearn.metrics.precision_recall_fscore_support(testClassConverted,predRidge))\n",
    "print(\"===================\")\n",
    "print(\"Precision | F1 Scores: \")\n",
    "print(\"===================\")\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = sklearn.metrics.confusion_matrix(testClassConverted,predRidge)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"================\")\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "alphaValues = [0.1,1,3,10,33,100,333,1000, 3333, 10000, 33333]\n",
    "meanAccuracyScores = []\n",
    "accuracyScores = []\n",
    "for i in range(0,len(alphaValues)):\n",
    "    currentAlphaValue = alphaValues[i]\n",
    "\n",
    "    for j in range(0,100):\n",
    "        rand = random.randint(0,50)\n",
    "        currentSet = trainData[rand:rand+40]\n",
    "        currentClass = trainClassConverted[rand:rand+40]\n",
    "        \n",
    "\n",
    "        regressor = Lasso(random_state=0,alpha=currentAlphaValue)\n",
    "        regressor.fit(currentSet,np.array(currentClass))\n",
    "        predLassoCV = regressor.predict(currentSet)\n",
    "        predLassoCV = np.around(predLassoCV)\n",
    "        score = np.sqrt(metrics.mean_squared_error(predLassoCV,currentClass))\n",
    "        accuracyScores.append(np.sqrt(metrics.mean_squared_error(predLassoCV,currentClass)))\n",
    "#         accuracyScores.append((sklearn.metrics.accuracy_score(currentClass,predLassoCV))*100)\n",
    "    meanAccuracyScores.append(np.mean(accuracyScores[i]))\n",
    "\n",
    "print(meanAccuracyScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAccuracyHyperParameter = meanAccuracyScores.index(min(meanAccuracyScores))\n",
    "bestHyperParameter = alphaValues[maxAccuracyHyperParameter]\n",
    "print(\"The Best HyperParameter Alpha is: \",bestHyperParameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 Regression with Best Hyperparameter determined using Cross Validation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "\n",
    "regressor = Lasso(random_state=0,alpha=bestHyperParameter)\n",
    "regressor.fit(trainData,trainClassConverted)\n",
    "predLasso = regressor.predict(testData)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(predLasso,testClassConverted))\n",
    "\n",
    "print(\"Final Root Mean Square Error: \",(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predLasso = np.around(predLasso)\n",
    "print(\"Accuracy: \",(sklearn.metrics.accuracy_score(testClassConverted,predLasso))*100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score | F1 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (sklearn.metrics.precision_recall_fscore_support(testClassConverted,predLasso))\n",
    "print(\"===================\")\n",
    "print(\"Precision | F1 Scores: \")\n",
    "print(\"===================\")\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = sklearn.metrics.confusion_matrix(testClassConverted,predLasso)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"================\")\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression with L2(Ridge) Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import random \n",
    "lambdaValues = [0.001, 0.003, 0.01, 0.03, 0.1,0.3,1,3,10,33]\n",
    "meanAccuracyScores = []\n",
    "accuracyScores = []\n",
    "currentLambdaValue = []\n",
    "for i in range(0,len(lambdaValues)):\n",
    "    currentLambdaValue = lambdaValues[i]\n",
    "\n",
    "    for j in range(0,100):\n",
    "        rand = random.randint(0,50)\n",
    "        currentSet = trainData[rand:rand+90]\n",
    "        currentClass = trainClassConverted[rand:rand+90]\n",
    "        regressor = Ridge(alpha=currentLambdaValue)\n",
    "\n",
    "        regressor.fit(currentSet,currentClass)\n",
    "        predRidgeCV = regressor.predict(currentSet)\n",
    "        predRidgeCV = np.around(predRidgeCV)\n",
    "        score = np.sqrt(metrics.mean_squared_error(predRidgeCV,currentClass))\n",
    "#         accuracyScores.append(np.sqrt(metrics.mean_squared_error(currentClass,predRidgeCV)))\n",
    "        accuracyScores.append((sklearn.metrics.accuracy_score(currentClass,predRidgeCV))*100)\n",
    "    meanAccuracyScores.append(np.mean(accuracyScores[i]))\n",
    "\n",
    "print(meanAccuracyScores)\n",
    "\n",
    "maxAccuracyHyperParameter = meanAccuracyScores.index(max(meanAccuracyScores))\n",
    "bestHyperParameterL = lambdaValues[maxAccuracyHyperParameter]\n",
    "print(\"The Best HyperParameter Lambda is: \",bestHyperParameterL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "regressor = Ridge(alpha=bestHyperParameterL)\n",
    "\n",
    "regressor.fit(trainData,trainClassConverted)\n",
    "predRidge = regressor.predict(testData)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(predRidge,testClassConverted))\n",
    "\n",
    "print(\"Final Root Mean Square Error: \",(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predRidge = np.around(predRidge)\n",
    "print(\"Accuracy: \",(sklearn.metrics.accuracy_score(testClassConverted,predRidge))*100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (sklearn.metrics.precision_recall_fscore_support(testClassConverted,predRidge))\n",
    "print(\"===================\")\n",
    "print(\"Precision | F1 Scores: \")\n",
    "print(\"===================\")\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = sklearn.metrics.confusion_matrix(testClassConverted,predRidge)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"================\")\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Top 5 Features using Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task-B: Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNISTData = pd.DataFrame(pd.read_csv(\"reduced_mnist.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = MNISTData['label']\n",
    "X = MNISTData.drop(['label'],axis=1).copy()\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X,Y,\n",
    "test_size=1/7.0, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(MNISTData))\n",
    "print(len(list(MNISTData)))\n",
    "co_df = pd.concat([MNISTData])\n",
    "with open(\"reduced_mnist.csv\") as fin:\n",
    "    fin.next()\n",
    "    total = sum(int(r[1]) for r in csv.reader(fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1Training using OneVsRest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsMNIST = OneVsRestClassifier(LinearSVC(random_state=0)).fit(trainX, trainY).predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.sqrt(metrics.mean_squared_error(predictionsMNIST,testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Root Mean Squared Error: \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",(sklearn.metrics.accuracy_score(testY,predictionsMNIST))*100,\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (sklearn.metrics.precision_recall_fscore_support(testY,predictionsMNIST))\n",
    "print(\"===================\")\n",
    "print(\"Precision | F1 Scores: \")\n",
    "print(\"===================\")\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = sklearn.metrics.confusion_matrix(testY,predictionsMNIST)\n",
    "\n",
    "print(\"================\")\n",
    "print(\"Confusion Matrix\")\n",
    "print(\"================\")\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choosing the best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "alphaValuesMNIST = [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]\n",
    "meanAccuracyScores = []\n",
    "accuracyScores = []\n",
    "currentAlphaValueMNIST = []\n",
    "for i in range(0,len(lambdaValues)):\n",
    "    currentLambdaValue = lambdaValues[i]\n",
    "\n",
    "    for j in range(0,10):\n",
    "        rand = random.randint(0,10)\n",
    "        currentSetMNIST = trainX[rand:rand+200]\n",
    "        currentClassMNIST = trainY[rand:rand+200]\n",
    "        regressor = Ridge(alpha=currentLambdaValue)\n",
    "\n",
    "        regressor.fit(currentSetMNIST,currentClassMNIST)\n",
    "        predMNISTCV = regressor.predict(currentSetMNIST)\n",
    "        predMNISTCV = np.around(predMNISTCV)\n",
    "        score = np.sqrt(metrics.mean_squared_error(predMNISTCV,currentClassMNIST))\n",
    "#         accuracyScores.append(np.sqrt(metrics.mean_squared_error(currentClassMNIST,predMNISTCV)))\n",
    "        accuracyScores.append((sklearn.metrics.accuracy_score(currentClassMNIST,predMNISTCV))*100)\n",
    "    meanAccuracyScores.append(np.mean(accuracyScores[i]))\n",
    "\n",
    "print(meanAccuracyScores)\n",
    "\n",
    "maxAccuracyHyperParameter = meanAccuracyScores.index(max(meanAccuracyScores))\n",
    "bestHyperParameterMNIST = alphaValuesMNIST[maxAccuracyHyperParameter]\n",
    "print(\"The Best HyperParameter Lambda is: \",bestHyperParameterMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training over best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regressor = linear_model.Ridge (alpha = bestHyperParameterMNIST)\n",
    "regressor.fit(trainX,np.array(trainY))\n",
    "predLassoCV = regressor.predict(testX)\n",
    "predLassoCV = np.around(predLassoCV)\n",
    "score = np.sqrt(metrics.mean_squared_error(predLassoCV,testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error: \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting average training accuracy vs average validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyScoreMNIST = ((sklearn.metrics.accuracy_score(testY,predLassoCV))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(accuracyScoreMNIST,accuracyScores[0],color='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(accuracyScoreMNIST,accuracyScores[1],color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model is overfitting as it is showing 100 % accuracy in cross validation set and around 20 % on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
